{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train[:1%]\")\n",
    "\n",
    "print(ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "fever_unified = []\n",
    "\n",
    "with open(\"train.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        if item['label'] != \"SUPPORTS\":\n",
    "            continue  \n",
    "\n",
    "        for eg_idx, evidence_group in enumerate(item['evidence']):\n",
    "            for ev_idx, ev in enumerate(evidence_group):\n",
    "                article_title = ev[2] if ev[2] is not None else \"NA\"\n",
    "                passage_id = f\"fever_{item['id']}_{eg_idx}_{ev_idx}\"\n",
    "                entry = {\n",
    "                    \"id\": passage_id,\n",
    "                    \"url\": f\"https://en.wikipedia.org/wiki/{article_title}\" if article_title != \"NA\" else \"NA\",\n",
    "                    \"title\": article_title,\n",
    "                    \"text\": item['claim']  \n",
    "                }\n",
    "                fever_unified.append(entry)\n",
    "\n",
    "print(f\"Total SUPPORTS entries: {len(fever_unified)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e963b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fever_unified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bebdedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fev_ds = pd.DataFrame(fever_unified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_wiki = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80224abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_wiki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24549b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ret = pd.concat([ds_wiki, fev_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e954064",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef8569",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ret_texts_list = df_ret['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_ret_texts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n",
    "API_key = os.getenv(\"GOOGLE_FACT_CHECK_API\")\n",
    "\n",
    "\n",
    "url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
    "\n",
    "params = {\n",
    "    'key': API_key,\n",
    "    'query': 'covid vaccine', \n",
    "    'languageCode': 'en-US',\n",
    "    'pageSize': 100\n",
    "}\n",
    "\n",
    "response = requests.get(url,params=params)\n",
    "len(response.json()['claims'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ddf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['claims'][0]\n",
    "i=0\n",
    "for claim in response.json()['claims']:\n",
    "    # print(claim.get('claimReview')[0].get('title',''))\n",
    "    print(claim\n",
    "          )\n",
    "    i+=1\n",
    "    if i==4:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9671c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_google_facts(query,num_iter = 1, pages = 100):\n",
    "    \n",
    "    load_dotenv()\n",
    "    API_key_google = os.getenv(\"GOOGLE_FACT_CHECK_API\")\n",
    "\n",
    "\n",
    "    url_google = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
    "\n",
    "    ds=[]\n",
    "    next_page_token = None\n",
    "    for _ in range(num_iter):\n",
    "\n",
    "        params = {\n",
    "        'key': API_key_google,\n",
    "        'query': query, \n",
    "        'languageCode': 'en-US',\n",
    "        'pageSize': pages\n",
    "        }\n",
    "\n",
    "        if next_page_token:\n",
    "            params['pageToken'] = next_page_token\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url_google,params=params)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            for claim in response.json()['claims']:\n",
    "                date = claim.get('claimReview',[])[0].get('reviewDate','').split('T')[0]\n",
    "                ds.append(\n",
    "                    {'title':claim.get('claimReview',[])[0].get('title',''),\n",
    "                     'text':claim.get('text',''),\n",
    "                     'url':claim.get('claimReview',[])[0].get('url',''), \n",
    "                     'Published_Date':date if date else \"No date available\",\n",
    "                     'source':claim.get('claimReview',[])[0].get('publisher',{}).get('name',\"No source available\")\n",
    "                     })\n",
    "                \n",
    "            next_page_token = response.json().get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing response: {e}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Google fetched {len(ds)} articles\")\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = fetch_google_facts('indian government')\n",
    "ds1,len(ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2435feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = fetch_google_facts('india')\n",
    "len(ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce359d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds1 + ds2\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(ds)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a191f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url_news_api = \"https://newsapi.org/v2/everything\"\n",
    "\n",
    "load_dotenv()\n",
    "api_key_news = os.getenv('NEWS_API')\n",
    "\n",
    "sort_by  =['relevancy', 'popularity', 'publishedAt']\n",
    "params = {\n",
    "    \"apiKey\":api_key_news,\n",
    "    \"q\":'indian actors',\n",
    "    \"sortBY\":sort_by[0],\n",
    "    \"pageSize\":69\n",
    "\n",
    "}\n",
    "\n",
    "res = requests.get(url_news_api,params=params)\n",
    "res.json().get('articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317bb34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arl=res.json().get('articles')\n",
    "arl[0]\n",
    "len(arl)\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2215e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_news_org(query: str, page_size: int = 100, num_iter: int = 12, sort_by_index: int = 0):\n",
    "    url_news_api = \"https://newsapi.org/v2/everything\"\n",
    "\n",
    "    load_dotenv()\n",
    "    api_key_news = os.getenv(\"NEWS_API\")\n",
    "    if not api_key_news:\n",
    "        raise ValueError(\"NEWS_API environment variable not set\")\n",
    "\n",
    "    sort_by = [\"relevancy\", \"popularity\", \"publishedAt\"]\n",
    "    if sort_by_index < 0 or sort_by_index >= len(sort_by):\n",
    "        sort_by_index = 0\n",
    "\n",
    "    news_ds = []\n",
    "\n",
    "    for page in range(1, num_iter + 1):\n",
    "        params = {\n",
    "            \"apiKey\": api_key_news,\n",
    "            \"q\": query,\n",
    "            \"sortBy\": sort_by[sort_by_index], \n",
    "            \"pageSize\": min(page_size, 100),\n",
    "            \"page\": page,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            res = requests.get(url_news_api, params=params, timeout=30)\n",
    "            res.raise_for_status()\n",
    "\n",
    "            if res.json().get(\"status\") == \"ok\":\n",
    "                print(f\"status | {res.json()['status']}\")\n",
    "\n",
    "            data = res.json().get(\"articles\")\n",
    "            if not data:\n",
    "                print(f\"Could not find any article at page {page}\")\n",
    "                break\n",
    "\n",
    "            for article in data:\n",
    "                if not article.get(\"content\"):\n",
    "                    continue\n",
    "\n",
    "                news_ds.append({\n",
    "                    \"title\": article.get(\"title\", \"\"),\n",
    "                    \"text\": (article.get(\"content\", \"\") or \"\") + (article.get(\"description\", \"\") or \"\"),\n",
    "                    \"url\": article.get(\"url\", \"\"),\n",
    "                    \"source\": article.get(\"source\", {}).get(\"name\", \"No source available\"),\n",
    "                    \"Published_Date\": article.get(\"publishedAt\", \"\"),\n",
    "                })\n",
    "\n",
    "            if len(news_ds) >= res.json().get(\"totalResults\", 0):\n",
    "                print(f\"No More Results | reached {res.json().get('totalResults', 0)} Results\")\n",
    "                break\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error on page(iteration) {page}: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error on page(iter) {page}: {e}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Fetched {len(news_ds)} news articles\")\n",
    "    return news_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d538f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "news_db = fetch_news_org('conjuring',500,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e774b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_db[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb3efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Referer\": \"https://www.ndtv.com/topics\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    \"Sec-Fetch-User\": \"?1\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "url=\"https://archives.ndtv.com/articles/2025-01.html\"\n",
    "\n",
    "response  = requests.get(url,headers=headers)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21493af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ndtv_rss_feeds():\n",
    "   \n",
    "    rss_feeds = {\n",
    "        'top_stories': 'https://feeds.feedburner.com/ndtvnews-top-stories',\n",
    "        'india': 'https://feeds.feedburner.com/ndtvnews-india-news',\n",
    "        'world': 'https://feeds.feedburner.com/ndtvnews-world-news',\n",
    "        'sports': 'https://feeds.feedburner.com/ndtvnews-sports',\n",
    "        'entertainment': 'https://feeds.feedburner.com/ndtvnews-entertainment',\n",
    "        'business': 'https://feeds.feedburner.com/ndtvnews-business'\n",
    "    }\n",
    "    \n",
    "    all_articles = []\n",
    "    \n",
    "    for category, url in rss_feeds.items():\n",
    "        try:\n",
    "            print(f\"Fetching RSS feed: {category}\")\n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch RSS feed: {response.status_code}\")\n",
    "                continue\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            items = soup.find_all('item')\n",
    "            \n",
    "            for item in items:\n",
    "                title = item.find('title')\n",
    "                link = item.find('link')\n",
    "                description = item.find('description')\n",
    "                pub_date = item.find('pubDate')\n",
    "                \n",
    "                if title and link:\n",
    "                    all_articles.append({\n",
    "                        'title': title.get_text().strip(),\n",
    "                        'url': link.get_text().strip(),\n",
    "                        'text': description.get_text().strip() if description else '',\n",
    "                        'Published_Date': pub_date.get_text().strip() if pub_date else '',\n",
    "                        'source': 'NDTV RSS'\n",
    "                    })\n",
    "            \n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing RSS feed {category}: {e}\")\n",
    "    \n",
    "    print(f\"Found {len(all_articles)} articles from RSS feeds\")\n",
    "    return all_articles\n",
    "\n",
    "rss_articles = get_ndtv_rss_feeds()\n",
    "rss_df = pd.DataFrame(rss_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49195b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_url = \"https://ddnews.gov.in/all-news-archive/\" \n",
    "\n",
    "res = requests.get(xml_url)\n",
    "\n",
    "res\n",
    "soup = BeautifulSoup(res.text , 'html.parser')\n",
    "a=soup.find('div',class_= 'moreStoriesItem')\n",
    "# url = a.a['href']\n",
    "# a.img['alt']\n",
    "# type(url)\n",
    "# # res = requests.get(url)\n",
    "# # res\n",
    "# url\n",
    "# soup.find_all('div')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37201d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.find('div',class_='moreStoriesText').text.strip().split('|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "soap = BeautifulSoup(res.text , 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae65588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dd_news_ugly(max_stories = 10):\n",
    "    dd_url = \"https://ddnews.gov.in/all-news-archive/\" \n",
    "    d=[]\n",
    "    try:\n",
    "        res = requests.get(dd_url)\n",
    "        res.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(res.text , 'html.parser')\n",
    "        stories=soup.find_all('div',class_= 'moreStoriesItem')\n",
    "\n",
    "        \n",
    "        for idx,story in enumerate(stories):\n",
    "            article_url  = story.a['href']\n",
    "            title = story.img['alt']\n",
    "            url_image  =story.img['src']\n",
    "            publishedtime = story.find('div',class_='moreStoriesText').text.strip().split('|')[0]\n",
    "\n",
    "            try:\n",
    "                print(f'Fetching article {idx+1}')\n",
    "                res_article = requests.get(article_url)\n",
    "                res_article.raise_for_status()\n",
    "\n",
    "                soap = BeautifulSoup(res_article.text , 'html.parser')\n",
    "                paras = soap.find('div',class_ = 'entry-content').find_all('p')\n",
    "\n",
    "                full_para = \"\"\n",
    "                for para in paras:\n",
    "                    full_para += para.text\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f'Could not fetch article {idx}')\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            \n",
    "            d.append({\n",
    "                'title':title,\n",
    "                'text':full_para,\n",
    "                'url':article_url,\n",
    "                'Published_Date':publishedtime,\n",
    "                'source':\"DD  News\"\n",
    "                \n",
    "                })\n",
    "            \n",
    "            if idx+1>=max_stories:\n",
    "                 break\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "            print('Could not fetch the link',e)\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(f\"Total Articles Fetched {len(d)}\")\n",
    "\n",
    "    return d\n",
    "\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "def fetch_dd_news_fixed(max_stories=10, delay=1):\n",
    "    base_url = \"https://ddnews.gov.in\"\n",
    "    dd_url = \"https://ddnews.gov.in/all-news-archive/\"\n",
    "    articles = []\n",
    "\n",
    "    try:\n",
    "        res = requests.get(dd_url)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        stories = soup.find_all('div', class_='moreStoriesItem')\n",
    "\n",
    "        for idx, story in enumerate(stories):\n",
    "            if idx >= max_stories:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                # Links and images (handle relative paths)\n",
    "                article_url = urljoin(base_url, story.a['href']) if story.a else None\n",
    "                title = story.img.get('alt', 'No title') if story.img else 'No title'\n",
    "                url_image = urljoin(base_url, story.img['src']) if story.img and story.img.get('src') else ''\n",
    "\n",
    "                publishedtime = (\n",
    "                    story.find('div', class_='moreStoriesText').text.strip().split('|')[0]\n",
    "                    if story.find('div', class_='moreStoriesText') else \"Unknown date\"\n",
    "                )\n",
    "\n",
    "                if not article_url:\n",
    "                    continue\n",
    "\n",
    "                # Fetch article page\n",
    "                time.sleep(delay)  # be nice to server\n",
    "                res_article = requests.get(article_url, timeout=15)\n",
    "                res_article.raise_for_status()\n",
    "                soap = BeautifulSoup(res_article.text, 'html.parser')\n",
    "\n",
    "                # Extract content\n",
    "                content_div = soap.find('div', class_='entry-content') or soap.find('div', class_='article-content')\n",
    "                if content_div:\n",
    "                    paras = content_div.find_all('p')\n",
    "                    full_para = \" \".join(p.get_text(strip=True) for p in paras)\n",
    "                else:\n",
    "                    full_para = soap.get_text(strip=True)\n",
    "\n",
    "                articles.append({\n",
    "                    'title': title,\n",
    "                    'text': full_para,\n",
    "                    'url': article_url,\n",
    "                    'Published_Date': publishedtime,\n",
    "                    'source': \"DD News\"\n",
    "                })\n",
    "\n",
    "                print(f\"Fetched article {idx+1}: {title[:50]}...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching article {idx+1}: {e}\")\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching archive: {e}\")\n",
    "\n",
    "    print(f\"âœ… Total Articles Fetched: {len(articles)}\")\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = fetch_dd_news_ugly(3)\n",
    "dd_db = pd.DataFrame(x)\n",
    "dd_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c44709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = fetch_dd_news_fixed(3)\n",
    "dd_db_a = pd.DataFrame(x)\n",
    "dd_db_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de66294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def striping(example):\n",
    "    return ' '.join(example.split('\\xa0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_db['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59773d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_db['title'] = dd_db['title'].apply(lambda x : striping(x) )\n",
    "dd_db['text'] = dd_db['text'].apply(lambda x : striping(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc88b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_db['title'][0]\n",
    "dd_db['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42563358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# def fetch_google_facts(query,num_iter = 1, pages = 100):\n",
    "# def fetch_dd_news(max_articles=20, delay=1):\n",
    "# def fetch_news_org(query:str , page_size:int = 100,num_iter = 12,sort_by_index:int = 0):\n",
    "# df_ret --> wiki dataset\n",
    "\n",
    "class fetch_all:\n",
    "    def __init__(self, num_pages = 100 , num_iter = 1):\n",
    "        self.num_pages  =num_pages\n",
    "        self.num_iter  = num_iter\n",
    "        self.articles  = []\n",
    "\n",
    "    def dd_news(self,max_articles):\n",
    "        print(\"DD News fetching, Just Pray their Server Dont die\")\n",
    "        try:\n",
    "            self.articles.extend(fetch_dd_news_fixed(max_articles))\n",
    "        except Exception  as e:\n",
    "            print(\"DD News failed | \", e)\n",
    "\n",
    "    def google(self,query = 'india'):\n",
    "        print(\"Google Fetching\")\n",
    "        self.articles.extend(fetch_google_facts(query , self.num_iter , self.num_pages))\n",
    "\n",
    "    def news_org(self, query = 'india' ,sort_idx = 0):\n",
    "        print(\"News API fetching\")\n",
    "        self.articles.extend(fetch_news_org(query=query , page_size=self.num_pages , num_iter=self.num_iter , sort_by_index=sort_idx))\n",
    "\n",
    "    def wiki(self):\n",
    "        print(\"Wiki fetching\")\n",
    "        self.articles.extend(df_ret.to_dict('records'))\n",
    "\n",
    "    def to_pandas(self):\n",
    "        return pd.DataFrame(self.articles)\n",
    "    \n",
    "    def ndtv(self):\n",
    "        self.articles.extend(get_ndtv_rss_feeds())\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.articles)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a04cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = fetch_all(100,500)\n",
    "db.news_org('india',0)\n",
    "db.wiki()\n",
    "# db.google('indian')\n",
    "db.ndtv()\n",
    "df = db.to_pandas()\n",
    "djson = db.to_json()\n",
    "db.dd_news(10000)\n",
    "df = db.to_pandas()\n",
    "djson = db.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe22638",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.dd_news(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e819ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56cf595",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/115.0.0.0 Safari/537.36\"\n",
    "                      }\n",
    "\n",
    "params = {\n",
    "    \"q\":'india',\n",
    "    \"tbm\":\"nws\",\n",
    "    'start':10\n",
    "    }\n",
    "\n",
    "search_url = 'https://www.google.com/search'\n",
    "\n",
    "res = requests.get(search_url,params=params,headers=headers)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e06b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "t= soup.find('div',class_='SoaBEf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=soup.select_one(\"div.SoaBEf a\")\n",
    "# for ar in m:\n",
    "#     p =ar.find('div',class_= \"n0jPhd ynAwRc MBeuO nDgy9d\").text\n",
    "#     n = m.find('div',class_ = \"GI74Re nDgy9d\").text\n",
    "#     title  =p+n\n",
    "#     url = \n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c290bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.find('div', class_ = \"n0jPhd ynAwRc MBeuO nDgy9d\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f29ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.find('div',class_ = \"GI74Re nDgy9d\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e9fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.find('div',class_=\"OSrXXb rbYSKb LfVVr\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0146471",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.find('div',class_ = \"MgUUmf NUnG9d\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c1878",
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_url = m['href']\n",
    "arc_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b282e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafilatura as tra\n",
    "\n",
    "down = tra.fetch_url(arc_url)\n",
    "content = tra.extract(down) if down else \"none extracted\"\n",
    "content = content if content else \"No content extracted\"\n",
    "\n",
    "content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = requests.get(arc_url,headers=headers)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "soap = BeautifulSoup(w.text,'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c299af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soap.find('h2').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f1d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_google_search(query:str = 'india',num_pages:int = 1):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/115.0.0.0 Safari/537.36\"\n",
    "                      }\n",
    "    \n",
    "    articles_gg= []\n",
    "    for pages in range(num_pages):\n",
    "        params = {\n",
    "            \"q\":query,\n",
    "            \"tbm\":\"nws\",\n",
    "            'start':pages * 10 \n",
    "            }\n",
    "\n",
    "        search_url = 'https://www.google.com/search'\n",
    "\n",
    "        try:\n",
    "            res = requests.get(search_url,params=params,headers=headers)\n",
    "            soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "            article_list=soup.select(\"div.SoaBEf a\")\n",
    "            if not article_list:\n",
    "                print(\"None Articles found\")\n",
    "            for article in article_list:\n",
    "                h1 = article.find('div',class_= \"n0jPhd ynAwRc MBeuO nDgy9d\").text\n",
    "                h2 = article.find('div',class_ = \"GI74Re nDgy9d\").text\n",
    "                title = h1 + h2\n",
    "\n",
    "                a_url = article['href']\n",
    "                time = article.find('div',class_=\"OSrXXb rbYSKb LfVVr\").text\n",
    "                source  = article.find('div',class_ = \"MgUUmf NUnG9d\").text\n",
    "\n",
    "                try:\n",
    "                    down = tra.fetch_url(a_url)\n",
    "                    content = tra.extract(down) if down else \"none extracted\"\n",
    "                    content = content if content else \"No content extracted\"\n",
    "                except Exception as e:\n",
    "                    content = f\"Error: {e}\"\n",
    "\n",
    "                articles_gg.append({\n",
    "                    \"title\":title,\n",
    "                    'url':a_url,\n",
    "                    'text':content,\n",
    "                    'pblished_date':time,\n",
    "                    'source':source\n",
    "                })\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error Fething Google search | {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unforseen Error | {e}\")\n",
    "\n",
    "    return articles_gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d65217",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = fetch_google_search(num_pages=30)\n",
    "dc = pd.DataFrame(s)\n",
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66afe2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc[dc['text']=='none extracted'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc1190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(db.articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61299576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urlparse, quote_plus\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_google_search_better(query: str = 'india', num_pages: int = 1, delay: float = 2.0):\n",
    "  \n",
    "    user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "    ]\n",
    "    \n",
    "    articles_gg = []\n",
    "    search_url = 'https://www.google.com/search'\n",
    "    \n",
    "    for page in range(num_pages):\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"tbm\": \"nws\",\n",
    "            'start': page * 10,\n",
    "            \"hl\": \"en\"  \n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            \"User-Agent\": random.choice(user_agents),\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"DNT\": \"1\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-Site\": \"none\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            time.sleep(delay + random.uniform(0, 1))\n",
    "            \n",
    "            res = requests.get(search_url, params=params, headers=headers, timeout=15)\n",
    "            res.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            \n",
    "            article_selectors = [\n",
    "                \"div.SoaBEf\",  \n",
    "                \"div.dbsr\",    \n",
    "                \"div.g\",       \n",
    "               \"div.tNxQIb\"   ]\n",
    "            \n",
    "            articles_found = False\n",
    "            for selector in article_selectors:\n",
    "                article_list = soup.select(selector)\n",
    "                if article_list:\n",
    "                    articles_found = True\n",
    "                    break\n",
    "            \n",
    "            if not articles_found:\n",
    "                print(f\"No articles found on page {page + 1}. Google may have blocked the request or changed their HTML structure.\")\n",
    "                # # Save the HTML for debugging\n",
    "                # with open(f\"google_page_{page+1}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                #     f.write(soup.prettify())\n",
    "                continue\n",
    "            \n",
    "            print(f\"Found {len(article_list)} articles on page {page + 1}\")\n",
    "            \n",
    "            for article in article_list:\n",
    "                try:\n",
    "                    title_elem = article.select_one(\".n0jPhd, .ynAwRc, .MBeuO, .nDgy9d, [role='heading']\")\n",
    "                    title = title_elem.get_text().strip() if title_elem else \"No title\"\n",
    "                    \n",
    "                    link_elem = article.find('a')\n",
    "                    a_url = link_elem.get('href') if link_elem else None\n",
    "                    \n",
    "                    if not a_url:\n",
    "                        continue\n",
    "                    \n",
    "                    if a_url.startswith('/url?q='):\n",
    "                        a_url = a_url.split('/url?q=')[1].split('&')[0]\n",
    "                        a_url = requests.utils.unquote(a_url)\n",
    "                    \n",
    "                    time_elem = article.select_one(\".OSrXXb, .rbYSKb, .LfVVr, .ZE0LJd\")\n",
    "                    time_text = time_elem.get_text().strip() if time_elem else \"Unknown date\"\n",
    "                    \n",
    "                    source_elem = article.select_one(\".MgUUmf, .NUnG9d, .IH8C7b\")\n",
    "                    source = source_elem.get_text().strip() if source_elem else \"Unknown source\"\n",
    "                    \n",
    "                    snippet_elem = article.select_one(\".GI74Re, .Y3v8qd, .l3AOke\")\n",
    "                    snippet = snippet_elem.get_text().strip() if snippet_elem else \"\"\n",
    "                    \n",
    "                    content = \"No content extracted\"\n",
    "                    try:\n",
    "                        article_res = requests.get(a_url, headers=headers, timeout=10)\n",
    "                        if article_res.status_code == 200:\n",
    "                            article_soup = BeautifulSoup(article_res.text, 'html.parser')\n",
    "                            \n",
    "                            content_selectors = [\n",
    "                                'article',\n",
    "                                'div.article-content',\n",
    "                                'div.story-content',\n",
    "                                'div.entry-content',\n",
    "                                'div.post-content',\n",
    "                                'div[class*=\"content\"]',\n",
    "                                'div[class*=\"body\"]',\n",
    "                                'main'\n",
    "                            ]\n",
    "                            \n",
    "                            for content_selector in content_selectors:\n",
    "                                content_elem = article_soup.select_one(content_selector)\n",
    "                                if content_elem:\n",
    "                                    paragraphs = content_elem.find_all('p')\n",
    "                                    if paragraphs:\n",
    "                                        content = \" \".join([p.get_text().strip() for p in paragraphs[:5]])  # First 5 paragraphs\n",
    "                                        break\n",
    "                            \n",
    "                            if content == \"No content extracted\":\n",
    "                                all_text = article_soup.get_text()\n",
    "                                content = \" \".join(all_text.split()[:200])  # First 200 words\n",
    "                    except Exception as e:\n",
    "                        content = f\"Error fetching content: {str(e)}\"\n",
    "                        if snippet and content.startswith(\"Error\"):\n",
    "                            content = snippet\n",
    "                    \n",
    "                    articles_gg.append({\n",
    "                        \"title\": title + snippet,\n",
    "                        'url': a_url,\n",
    "                        'text': content,\n",
    "                        'published_date': time_text,\n",
    "                        'source': source,\n",
    "                        \n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article: {e}\")\n",
    "                    continue\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error on page {page + 1}: {e}\")\n",
    "            # If we get blocked, break out of the loop\n",
    "            if \"429\" in str(e) or \"430\" in str(e):\n",
    "                print(\"Google is blocking requests. Stopping.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error on page {page + 1}: {e}\")\n",
    "    \n",
    "    print(f\"Total articles fetched from Google Search: {len(articles_gg)}\")\n",
    "    return articles_gg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0748582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = fetch_google_search_better('indian',30)\n",
    "dz=pd.DataFrame(z)\n",
    "dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz[dz['text']==\"No content extracted\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444cfde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def fetch_google_facts(query,num_iter = 1, pages = 100):\n",
    "# def fetch_dd_news(max_articles=20, delay=1):\n",
    "# def fetch_news_org(query:str , page_size:int = 100,num_iter = 12,sort_by_index:int = 0):\n",
    "# df_ret --> wiki dataset\n",
    "# def fetch_google_search_better(query: str = 'india', num_pages: int = 1, delay: float = 2.0):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d84bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "class DataAggregator:\n",
    "    def __init__(self, num_pages=100, num_iter=1):\n",
    "        self.num_pages = num_pages\n",
    "        self.num_iter = num_iter\n",
    "        self.articles = []\n",
    "        self.chunks = []\n",
    "        \n",
    "    def dd_news(self, max_articles=20):\n",
    "        print(\"Fetching DD News\")\n",
    "        try:\n",
    "            articles = fetch_dd_news_fixed(max_articles)\n",
    "            self.articles.extend(articles)\n",
    "            print(f\"Added {len(articles)} DD News articles\")\n",
    "        except Exception as e:\n",
    "            print(f\"DD News error: {e}\")\n",
    "        return self\n",
    "    \n",
    "    def google_facts(self, query=\"india\"):\n",
    "        print(\"Fetching Google Facts\")\n",
    "        try:\n",
    "            articles = fetch_google_facts(query, self.num_iter, self.num_pages)\n",
    "            self.articles.extend(articles)\n",
    "            print(f\"Added {len(articles)} Google Fact Check articles\")\n",
    "        except Exception as e:\n",
    "            print(f\"Google Facts error: {e}\")\n",
    "        return self\n",
    "    \n",
    "    def news_org(self, query=\"india\", sort_idx=0):\n",
    "        print(\"Fetching News API\")\n",
    "        try:\n",
    "            articles = fetch_news_org(query, self.num_pages, self.num_iter, sort_idx)\n",
    "            self.articles.extend(articles)\n",
    "            print(f\"Added {len(articles)} News API articles\")\n",
    "        except Exception as e:\n",
    "            print(f\"News API error: {e}\")\n",
    "        return self\n",
    "    \n",
    "    def wiki(self, wiki_df):\n",
    "        print(\"Fetching Wikipedia\")\n",
    "        try:\n",
    "            articles = wiki_df.to_dict('records')\n",
    "            self.articles.extend(articles)\n",
    "            print(f\"Added {len(articles)} Wikipedia articles\")\n",
    "        except Exception as e:\n",
    "            print(f\"Wikipedia error: {e}\")\n",
    "        return self\n",
    "    \n",
    "    def search(self, query=\"india\"):\n",
    "        print(\"Fetching Google Search\")\n",
    "        try:\n",
    "            articles = fetch_google_search_better(query, self.num_pages)\n",
    "            self.articles.extend(articles)\n",
    "            print(f\"Added {len(articles)} Google Search articles\")\n",
    "        except Exception as e:\n",
    "            print(f\"Google Search error: {e}\")\n",
    "        return self\n",
    "    \n",
    "    def ndtv(self):\n",
    "        print(\"Fetching NDTV RSS\")\n",
    "        try:\n",
    "            articles = get_ndtv_rss_feeds()\n",
    "            self.articles.extend(articles)\n",
    "            print(f\"Added {len(articles)} NDTV RSS articles\")\n",
    "        except Exception as e:\n",
    "            print(f\"NDTV RSS error: {e}\")\n",
    "        return self\n",
    "    \n",
    "    def chunk_text(self,text, chunk_size=500, overlap=50):\n",
    "\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(words):\n",
    "            end = start + chunk_size\n",
    "            chunk = \" \".join(words[start:end])\n",
    "            chunks.append(chunk)\n",
    "            start += chunk_size - overlap\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def create_chunks(self , chunk_size=500 , overlap=50):\n",
    "\n",
    "        self.chunks = []\n",
    "        for articles in self.articles:\n",
    "            text = articles.get('text',\"\")\n",
    "            if not text:\n",
    "                continue\n",
    "            chunked_text = self.chunked_text(text,chunk_size,overlap)\n",
    "\n",
    "            for idx,chu_text in enumerate(chunked_text):\n",
    "                chunk_data = articles.copy()\n",
    "                chunk_data['chunk_text'] = chu_text\n",
    "                chunk_data['chunk_text'] = chunk_text\n",
    "                chunk_data['chunk_id'] = f\"{article.get('url', '')}_{i}\"\n",
    "                chunk_data['is_chunk'] = True\n",
    "                chunk_data['chunk_number'] = i\n",
    "                chunk_data['total_chunks'] = len(text_chunks)\n",
    "                \n",
    "                self.chunks.append(chunk_data)\n",
    "        \n",
    "        print(f\"Created {len(self.chunks)} chunks from {len(self.articles)} articles\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_chunks_df(self):\n",
    "        \"\"\"Get chunks as DataFrame\"\"\"\n",
    "        return pd.DataFrame(self.chunks)\n",
    "    \n",
    "    def get_chunks_json(self):\n",
    "        \"\"\"Get chunks as JSON\"\"\"\n",
    "        return json.dumps(self.chunks)\n",
    "    \n",
    "    def save_chunks(self, filename, format=\"json\"):\n",
    "        \"\"\"Save chunks to file\"\"\"\n",
    "        if not self.chunks:\n",
    "            print(\"No chunks available. Run create_chunks() first.\")\n",
    "            return self\n",
    "        \n",
    "        try:\n",
    "            if format == \"json\":\n",
    "                with open(f\"{filename}_chunks.json\", \"w\") as f:\n",
    "                    json.dump(self.chunks, f, indent=2)\n",
    "            elif format == \"csv\":\n",
    "                pd.DataFrame(self.chunks).to_csv(f\"{filename}_chunks.csv\", index=False)\n",
    "            elif format == \"pkl\":\n",
    "                pd.DataFrame(self.chunks).to_pickle(f\"{filename}_chunks.pkl\")\n",
    "            \n",
    "            print(f\"Saved {len(self.chunks)} chunks to {filename}_chunks.{format}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Save chunks error: {e}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    \n",
    "    def to_pandas(self):\n",
    "        return pd.DataFrame(self.articles)\n",
    "    \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.articles)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.articles = []\n",
    "        print(\"Cleared all articles\")\n",
    "        return self\n",
    "    \n",
    "    def stats(self):\n",
    "        df = self.to_pandas()\n",
    "        return {\n",
    "            'total': len(self.articles),\n",
    "            'sources': df['source'].value_counts().to_dict() if 'source' in df.columns else {}\n",
    "        }\n",
    "    \n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"Remove duplicate articles based on title and URL\"\"\"\n",
    "        if not self.articles:\n",
    "            return self\n",
    "            \n",
    "        seen = set()\n",
    "        unique_articles = []\n",
    "        \n",
    "        for article in self.articles:\n",
    "            identifier = f\"{article.get('title', '')}_{article.get('url', '')}\"\n",
    "            identifier_hash = hashlib.md5(identifier.encode()).hexdigest()\n",
    "            \n",
    "            if identifier_hash not in seen:\n",
    "                seen.add(identifier_hash)\n",
    "                unique_articles.append(article)\n",
    "        \n",
    "        removed = len(self.articles) - len(unique_articles)\n",
    "        self.articles = unique_articles\n",
    "        \n",
    "        print(f\"Removed {removed} duplicate articles\")\n",
    "        return self\n",
    "    \n",
    "    def save(self, filename, format=\"json\"):\n",
    "        \"\"\"Save articles to file in specified format\"\"\"\n",
    "        try:\n",
    "            if format == \"json\":\n",
    "                with open(f\"{filename}.json\", \"w\") as f:\n",
    "                    json.dump(self.articles, f, indent=2)\n",
    "            elif format == \"csv\":\n",
    "                self.to_pandas().to_csv(f\"{filename}.csv\", index=False)\n",
    "            elif format == \"pkl\":\n",
    "                self.to_pandas().to_pickle(f\"{filename}.pkl\")\n",
    "            else:\n",
    "                raise ValueError(\"Format must be json, csv, or pkl\")\n",
    "                \n",
    "            print(f\"Saved {len(self.articles)} articles to {filename}.{format}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Save error: {e}\")\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c58b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregator = DataAggregator(num_pages=5, num_iter=2)\n",
    "\n",
    "# # Collect data from multiple sources\n",
    "# data = (aggregator\n",
    "#     .dd_news(10)\n",
    "#     .google_facts(\"covid\")\n",
    "#     .news_org(\"india\", 2)\n",
    "#     .search(\"current news\")\n",
    "#     .ndtv()\n",
    "#     .wiki(df_ret)\n",
    "#     .remove_duplicates()  # Remove duplicates\n",
    "#     .save(\"my_articles\", \"json\")  # Save as JSON\n",
    "#     .save(\"my_articles\", \"csv\")   # Save as CSV\n",
    "#     .to_pandas()\n",
    "# )\n",
    "\n",
    "# print(f\"Collected {len(data)} articles after deduplication\")\n",
    "# print(aggregator.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d20fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = DataAggregator(num_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9159a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.google_facts(\"india\").wiki(df_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537efa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.save('first',\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"India politics OR government OR parliament\",\n",
    "    \"Lok Sabha OR Rajya Sabha OR Indian election\",\n",
    "    \"India Supreme Court OR High Court\",\n",
    "\n",
    "    \"India economy OR GDP OR inflation\",\n",
    "    \"Reserve Bank of India OR RBI\",\n",
    "    \"India stock market OR Sensex OR Nifty\",\n",
    "    \"India startups OR unicorn OR funding\",\n",
    "\n",
    "    \"India artificial intelligence OR AI OR machine learning\",\n",
    "    \"ISRO OR Chandrayaan OR Gaganyaan\",\n",
    "    \"India 5G OR technology OR semiconductors\",\n",
    "    \"India renewable energy OR solar OR nuclear\",\n",
    "\n",
    "    \"India education OR schools OR universities\",\n",
    "    \"India healthcare OR hospitals OR vaccines\",\n",
    "    \"India poverty OR unemployment OR inequality\",\n",
    "    \"India climate change OR pollution OR environment\",\n",
    "\n",
    "    \"India Pakistan OR border OR Kashmir\",\n",
    "    \"India China OR LAC OR diplomacy\",\n",
    "    \"India defense OR military OR DRDO\",\n",
    "    \"India foreign policy OR G20 OR BRICS\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24381511",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in queries:\n",
    "    data.news_org(query=query,sort_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in queries:\n",
    "    data.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# path = Path.cwd()\n",
    "\n",
    "# for files in path.glob('*.html'):\n",
    "#     if files.is_file():\n",
    "#         try:\n",
    "#             files.unlink()\n",
    "#             print(f\"File '{files}' deleted successfully.\")\n",
    "#         except OSError as e:\n",
    "#             print(f\"Error deleting file '{files}': {e}\")\n",
    "#     else:\n",
    "#         print(f\"File '{files}' does not exist.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c33d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.create_chunks()\n",
    "# data.save_chunks('first','csv')\n",
    "# data.save_chunks('first')\n",
    "data.save('second','csv')\n",
    "data.save('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4909c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"\n",
    "    Split text into chunks of `chunk_size` tokens (approx by words here),\n",
    "    with `overlap` words overlapping.\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7497ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_data = []\n",
    "\n",
    "for article in data.articles:\n",
    "    content = article[\"text\"]\n",
    "    chunks = chunk_text(content, chunk_size=500, overlap=50)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_data.append({\n",
    "            \"title\": article[\"title\"],\n",
    "            \"url\": article[\"url\"],\n",
    "            \"Published_Date\": article.get(\"Published_Date\",\"Not Available\"),\n",
    "            \"source\": article.get(\"source\",'Not Available'),\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c53365",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4972643",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(chunk_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ad7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('second.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8613d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "arc = df.to_dict(orient='records')\n",
    "arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9faa884",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e900d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"\n",
    "    Split text into chunks of `chunk_size` tokens (approx by words here),\n",
    "    with `overlap` words overlapping.\n",
    "    \"\"\"\n",
    "    text  =str(text)\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f972d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_data = []\n",
    "\n",
    "for article in arc:\n",
    "    content = article[\"text\"]\n",
    "    chunks = chunk_text(content, chunk_size=500, overlap=50)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_data.append({\n",
    "            \"title\": article[\"title\"],\n",
    "            \"url\": article[\"url\"],\n",
    "            \"Published_Date\": article.get(\"Published_Date\",\"Not Available\"),\n",
    "            \"source\": article.get(\"source\",'Not Available'),\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk\n",
    "        })\n",
    "\n",
    "chunk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae83d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.DataFrame(chunk_data)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8f07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.to_csv('third_chunks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('final_chunks.json','w') as f:\n",
    "    json.dump(chunk_data,f,indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d5bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3af3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
